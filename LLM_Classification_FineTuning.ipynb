{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-13T01:25:41.082567Z",
     "start_time": "2025-08-13T01:25:40.788051Z"
    }
   },
   "source": [
    "import zipfile\n",
    "import os\n",
    "from pyexpat import features\n",
    "\n",
    "from lark.tools import flags\n",
    "\n",
    "zip_path = \"llm-classification-finetuning.zip\"\n",
    "extract_path = \"./kaggle\"\n",
    "\n",
    "# Create folder if it doesn't exist\n",
    "os.makedirs(extract_path, exist_ok=True)\n",
    "\n",
    "# Open and extract\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_path)\n",
    "\n",
    "print(f\"Unzipped to {extract_path}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unzipped to ./kaggle\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T01:26:51.538936Z",
     "start_time": "2025-08-13T01:26:41.711970Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "sample_submission = pd.read_csv(\"./kaggle/sample_submission.csv\")\n",
    "sample_submission"
   ],
   "id": "88713a14235af159",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        id  winner_model_a  winner_model_b  winner_tie\n",
       "0   136060        0.333333        0.333333    0.333333\n",
       "1   211333        0.333333        0.333333    0.333333\n",
       "2  1233961        0.333333        0.333333    0.333333"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>winner_model_a</th>\n",
       "      <th>winner_model_b</th>\n",
       "      <th>winner_tie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>136060</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>211333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1233961</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T01:31:25.703311Z",
     "start_time": "2025-08-13T01:31:24.590656Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_df = pd.read_csv(\"./kaggle/test.csv\")\n",
    "train_df = pd.read_csv(\"./kaggle/train.csv\")\n",
    "\n",
    "train_df.head(2)"
   ],
   "id": "4e48384d9c39e6e4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      id             model_a     model_b  \\\n",
       "0  30192  gpt-4-1106-preview  gpt-4-0613   \n",
       "1  53567           koala-13b  gpt-4-0613   \n",
       "\n",
       "                                              prompt  \\\n",
       "0  [\"Is it morally right to try to have a certain...   \n",
       "1  [\"What is the difference between marriage lice...   \n",
       "\n",
       "                                          response_a  \\\n",
       "0  [\"The question of whether it is morally right ...   \n",
       "1  [\"A marriage license is a legal document that ...   \n",
       "\n",
       "                                          response_b  winner_model_a  \\\n",
       "0  [\"As an AI, I don't have personal beliefs or o...               1   \n",
       "1  [\"A marriage license and a marriage certificat...               0   \n",
       "\n",
       "   winner_model_b  winner_tie  \n",
       "0               0           0  \n",
       "1               1           0  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>model_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "      <th>winner_model_a</th>\n",
       "      <th>winner_model_b</th>\n",
       "      <th>winner_tie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30192</td>\n",
       "      <td>gpt-4-1106-preview</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>[\"Is it morally right to try to have a certain...</td>\n",
       "      <td>[\"The question of whether it is morally right ...</td>\n",
       "      <td>[\"As an AI, I don't have personal beliefs or o...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53567</td>\n",
       "      <td>koala-13b</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>[\"What is the difference between marriage lice...</td>\n",
       "      <td>[\"A marriage license is a legal document that ...</td>\n",
       "      <td>[\"A marriage license and a marriage certificat...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T01:31:59.152352Z",
     "start_time": "2025-08-13T01:31:59.148521Z"
    }
   },
   "cell_type": "code",
   "source": "train_df.columns.to_list()",
   "id": "f53d56344829d168",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'model_a', 'model_b', 'prompt', 'response_a', 'response_b',\n",
       "       'winner_model_a', 'winner_model_b', 'winner_tie'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T01:55:47.649551Z",
     "start_time": "2025-08-13T01:55:47.641526Z"
    }
   },
   "cell_type": "code",
   "source": "train_df.dtypes",
   "id": "a1fa830e07f33a57",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                 int64\n",
       "model_a           object\n",
       "model_b           object\n",
       "prompt            object\n",
       "response_a        object\n",
       "response_b        object\n",
       "winner_model_a     int64\n",
       "winner_model_b     int64\n",
       "winner_tie         int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import log_loss\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "class OptimizedLLMPredictor:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the predictor with optimized setting\"\"\"\n",
    "        self.label_encoders = {}\n",
    "        self.scaler = StandardScaler()\n",
    "        self.models = {}\n",
    "        self.feature_names = []\n",
    "\n",
    "        print(\"OptimizedLLMPredictor Initialized!\")\n",
    "\n",
    "    def extract_fast_features(self, df: pd.DataFrame, is_train=True):\n",
    "        \"\"\"Extract optimized features with focus on speed and performance\"\"\"\n",
    "\n",
    "        print(\"Starting feature engineering...\")\n",
    "        features: pd.DataFrame = df[['id']].copy()\n",
    "\n",
    "        for resp in ['response_a', 'response_b']:\n",
    "            text_col = df[resp].astype(str)\n",
    "\n",
    "            # Basic length metrics\n",
    "            features[f'{resp}_len'] = text_col.str.len()\n",
    "            features[f'{resp}_words'] = text_col.str.split().str.len()\n",
    "            features[f'{resp}_sentences'] = text_col.str.count(r'[.!?]+')\n",
    "\n",
    "            # Advanced metrics\n",
    "            features[f'{resp}_avg_word_len'] = features[f'{resp}_len'] / (features[f'{resp}_words'] + 1)\n",
    "            features[f'{resp}_punct_ratio'] = text_col.str.count(r'\\w\\s') / (features[f'{resp}_len'] + 1)\n",
    "            features[f'{resp}_upper_ratio'] = text_col.str.count(r'[A-Z]') / (features[f'{resp}_len'] + 1)\n",
    "\n",
    "            # Structure indicators\n",
    "            features[f'{resp}_newlines'] = text_col.str.count(r\"\\n\")\n",
    "            features[f'{resp}_code_blocks'] = text_col.str.count(r\"```\")\n",
    "            features[f'{resp}_bullets'] = text_col.str.count(r'^\\s*[-*•]\\s', flags=re.MULTILINE)\n",
    "            features[f'{resp}_numbers'] = text_col.str.count(r'^\\s*\\d+\\.\\s', flags=re.MULTILINE)\n",
    "\n",
    "            # Quality indicators\n",
    "            features[f'{resp}_questions'] = text_col.str.count(r'\\?')\n",
    "            features[f'{resp}_exclamations'] = text_col.str.count(r'\\!')\n",
    "        print(\"Basic text features extracted\")\n",
    "\n",
    "        prompt_text = df['prompt'].astype(str)\n",
    "        features['prompt_len'] = prompt_text.str.len()\n",
    "        features['prompt_words'] = prompt_text.str.split().str.len()\n",
    "        features['prompt_questions'] = prompt_text.str.count(r'\\?')\n",
    "\n",
    "        print(\"Prompt features extracted\")\n",
    "\n",
    "        # Length comparisons\n",
    "        features['len_ratio_a_b'] = features['response_a_len'] / features['response_b_len']\n",
    "        features['len_diff_a_b'] = features['response_a_len'] - features['response_b_len']\n",
    "        features['word_ratio_a_b'] = features['response_a_words'] / (features['response_b_words'] + 1)\n",
    "        features['word_diff_a_b'] = features['response_a_words'] - features['response_b_words']\n",
    "\n",
    "        # Quality comparisons\n",
    "        features['struct_diff_a_b'] = (features['response_a_bullets'] + features['response_a_numbers']) - (\n",
    "                features['response_b_bullets'] + features['response_b_numbers'])\n",
    "        features['engagement_diff_a_b'] = (features['response_a_questions'] + features['response_a_exclamations']) - (\n",
    "                features['response_b_questions'] + features['response_b_exclamations'])\n",
    "        print(\"Comparative features extracted\")\n",
    "\n",
    "        if is_train and 'model_a' in df.columns:\n",
    "            # Fast model encoding\n",
    "            for model_col in ['model_a', 'model_b']:\n",
    "                if model_col not in self.label_encoders:\n",
    "                    self.label_encoders[model_col] = LabelEncoder()\n",
    "                    features[f'{model_col}_id'] = self.label_encoders[model_col].fit_transform(df[model_col])\n",
    "                    features[f'{model_col}_id'] = self.label_encoders[model_col].transform(df[model_col])\n",
    "                else:\n",
    "                    features[f'{model_col}_id'] = self.label_encoders[model_col].transform(df[model_col])\n",
    "\n",
    "            # Quick model stats\n",
    "            model_wins = df.groupby('model_a')['winner_model_a'].mean()\n",
    "            model_wins_b = df.groupby('model_b')['winner_model_b'].mean()\n",
    "\n",
    "            features['model_a_win_rate'] = df['model_a'].map(model_wins).fillna(0.33)\n",
    "            features['model_b_win_rate'] = df['model_b'].map(model_wins_b).fillna(0.33)\n",
    "\n",
    "            print(\"Model features extracted\")\n",
    "        elif not is_train:\n",
    "            # For test data, create dummy model features to maintain consistency\n",
    "            features['model_a_id'] = 0  # default encoding for unknown models\n",
    "            features['model_b_id'] = 0\n",
    "            features['model_a_win_rate'] = 0.33  # default win rate\n",
    "            features['model_b_win_rate'] = 0.33\n",
    "\n",
    "            print(\"Dummy model features added for test consistency\")\n",
    "\n",
    "        def fast_word_overlap(row):\n",
    "            words_a = set(str(row['response_a']).lower().split())\n",
    "            words_b = set(str(row['response_b']).lower().split())\n",
    "            if len(words_a) == 0 or len(words_b) == 0:\n",
    "                return 0\n",
    "            return len(words_a & words_b) / len(words_a | words_b)\n",
    "\n",
    "        features['word_overlap'] = df.apply(fast_word_overlap, axis=1)\n",
    "\n",
    "        print(\"Similarity features extracted\")\n",
    "\n",
    "        features = features.fillna(0)\n",
    "        self.feature_names = [col for col in features.columns if col != 'id']\n",
    "\n",
    "        print(f\" Feature engineering completed! Total features: {len(self.feature_names)}\")\n",
    "\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        return features\n",
    "\n",
    "    def train_optimized_ensemble(self, X, y):\n",
    "        \"\"\"Train optimized ensemble with focus on speed and performance\"\"\"\n",
    "\n",
    "        print(\"Training optimized ensemble models...\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        y_multiclass = np.argmax(y.values, axis=1)\n",
    "\n",
    "\n",
    "        # Model 1: LightGBM (Primary Model)\n",
    "        print(\"Training LightGBM (Primary Model) ...\")\n",
    "        self.models['lgb'] = lgb.LGBMClassifier(\n",
    "            n_estimators=800,\n",
    "            learning_rate=0.08,\n",
    "            max_depth=6,\n",
    "            num_leaves=31,\n",
    "            subsample=0.9,\n",
    "            colsample_bytree=0.9,\n",
    "            random_state=42,\n",
    "            objective='multiclass',\n",
    "            num_class=3,\n",
    "            verbose=-1,\n",
    "            force_col_wise=True # Optimization for speed\n",
    "        )\n",
    "        self.models['lgb'].fit(X, y_multiclass)\n",
    "\n",
    "        # Model 2: XGBoost (Secondary Model)\n",
    "        print(\"Training XGBoost\")\n",
    "        self.models['xgb'] = xgb.XGBClassifier(\n",
    "            n_estimators=600,\n",
    "            learning_rate=0.08,\n",
    "            max_depth=6,\n",
    "            subsample=0.9,\n",
    "            colsample_bytree=0.9,\n",
    "            random_state=42,\n",
    "            objective='multi:softprob',\n",
    "            eval_metric='mlogloss',\n",
    "            verbosity=0,\n",
    "            tree_method='hist' # Faster training\n",
    "        )\n",
    "\n",
    "        # Model 3: CatBoost (Robust Model)\n",
    "        print(\"Training CatBoost...\")\n",
    "\n",
    "\n"
   ],
   "id": "df36771e330f6ebb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
