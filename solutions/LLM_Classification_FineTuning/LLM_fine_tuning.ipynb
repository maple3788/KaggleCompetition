{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## This is from https://www.kaggle.com/code/arabidopsisthalian/fine-tune",
   "id": "ed519f247d8c9e82"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T15:06:51.441076Z",
     "start_time": "2025-08-14T15:06:51.435974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import gc\n",
    "import re\n",
    "import random\n",
    "from time import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification\n",
    "from peft import get_peft_model, PeftModel, PeftConfig, get_peft_config, LoraConfig, TaskType\n",
    "import torch.nn.functional as F\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "tqdm.pandas()\n",
    "llama3_path = os.getenv(\"llama3_path\")"
   ],
   "id": "881afa0f0c8fa075",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T14:36:00.413252Z",
     "start_time": "2025-08-14T14:36:00.385931Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CFG:\n",
    "    NUM_EPOCH = 1\n",
    "    BATCH_SIZE = 16\n",
    "    DROPOUT = 0.05\n",
    "    MODEL_NAME = 'meta-llama/Meta-Llama-3-8B'\n",
    "    # MODEL_NAME = '/kaggle/input/llama-3/transformers/8b-hf/1'\n",
    "    SEED = 2024\n",
    "    MAX_LENGTH = 1024\n",
    "    NUM_WARMUP_STEPS = 128\n",
    "    LR_MAX = 5e-5\n",
    "    NUM_LABELS = 3\n",
    "    LORA_RANK = 16\n",
    "    LORA_ALPHA = 32\n",
    "    LORA_MODULES = ['q_proj', 'v_proj']\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ],
   "id": "726609d4d3896c21",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T14:36:00.433681Z",
     "start_time": "2025-08-14T14:36:00.427007Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def set_seeds(seed):\n",
    "    \"\"\"Set seeds for reproducibility\"\"\"\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "set_seeds(seed=CFG.SEED)"
   ],
   "id": "d0d2c1aee0ccd912",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T15:06:59.816653Z",
     "start_time": "2025-08-14T15:06:59.313356Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(llama3_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.add_eos_token = True\n",
    "\n",
    "tokenizer.save_pretrained(\"tokenizer\")"
   ],
   "id": "46f54ed40adb654c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tokenizer\\\\tokenizer_config.json',\n",
       " 'tokenizer\\\\special_tokens_map.json',\n",
       " 'tokenizer\\\\chat_template.jinja',\n",
       " 'tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T14:36:00.978452Z",
     "start_time": "2025-08-14T14:36:00.975662Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_token_lengths(texts):\n",
    "    # tokenize and receive input_ids for reach text\n",
    "    input_ids = tokenizer(texts.tolist(), return_tensors='np')['input_ids']\n",
    "    # return length of inputs_ids for each text\n",
    "    return [len(t) for t in input_ids]"
   ],
   "id": "b632863c8525aa7d",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T14:36:02.256340Z",
     "start_time": "2025-08-14T14:36:01.014866Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train: pd.DataFrame = pd.read_csv('../../kaggle/LLM_Classification_FineTuning/train.csv')\n",
    "\n",
    "\n",
    "def process(input_str):\n",
    "    stripped_str = input_str.strip('[]')\n",
    "    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]\n",
    "    return ' '.join(sentences)\n",
    "\n",
    "\n",
    "train.loc[:, 'prompt'] = train['prompt'].apply(process)  # safer than train['prompt'] = ...\n",
    "train.loc[:, 'response_a'] = train['response_a'].apply(process)\n",
    "train.loc[:, 'response_b'] = train['response_b'].apply(process)\n",
    "\n",
    "# Drop 'Nul' for training\n",
    "indexes = train[(train.response_a == 'null') & (train.response_b == 'null')].index\n",
    "train.drop(indexes, inplace=True)\n",
    "train.reset_index(inplace=True, drop=True)"
   ],
   "id": "93d4a7977445c251",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T14:36:02.299059Z",
     "start_time": "2025-08-14T14:36:02.291962Z"
    }
   },
   "cell_type": "code",
   "source": "train.head(5)",
   "id": "6806cd3dca507508",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       id             model_a              model_b  \\\n",
       "0   30192  gpt-4-1106-preview           gpt-4-0613   \n",
       "1   53567           koala-13b           gpt-4-0613   \n",
       "2   65089  gpt-3.5-turbo-0613       mistral-medium   \n",
       "3   96401    llama-2-13b-chat  mistral-7b-instruct   \n",
       "4  198779           koala-13b   gpt-3.5-turbo-0314   \n",
       "\n",
       "                                              prompt  \\\n",
       "0  Is it morally right to try to have a certain p...   \n",
       "1  What is the difference between marriage licens...   \n",
       "2  explain function calling. how would you call a...   \n",
       "3  How can I create a test set for a very rare ca...   \n",
       "4  What is the best way to travel from Tel-Aviv t...   \n",
       "\n",
       "                                          response_a  \\\n",
       "0  The question of whether it is morally right to...   \n",
       "1  A marriage license is a legal document that al...   \n",
       "2  Function calling is the process of invoking or...   \n",
       "3  Creating a test set for a very rare category c...   \n",
       "4  The best way to travel from Tel Aviv to Jerusa...   \n",
       "\n",
       "                                          response_b  winner_model_a  \\\n",
       "0  As an AI, I don't have personal beliefs or opi...               1   \n",
       "1  A marriage license and a marriage certificate ...               0   \n",
       "2  Function calling is the process of invoking a ...               0   \n",
       "3  When building a classifier for a very rare cat...               1   \n",
       "4  The best way to travel from Tel-Aviv to Jerusa...               0   \n",
       "\n",
       "   winner_model_b  winner_tie  \n",
       "0               0           0  \n",
       "1               1           0  \n",
       "2               0           1  \n",
       "3               0           0  \n",
       "4               1           0  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>model_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "      <th>winner_model_a</th>\n",
       "      <th>winner_model_b</th>\n",
       "      <th>winner_tie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30192</td>\n",
       "      <td>gpt-4-1106-preview</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>Is it morally right to try to have a certain p...</td>\n",
       "      <td>The question of whether it is morally right to...</td>\n",
       "      <td>As an AI, I don't have personal beliefs or opi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53567</td>\n",
       "      <td>koala-13b</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>What is the difference between marriage licens...</td>\n",
       "      <td>A marriage license is a legal document that al...</td>\n",
       "      <td>A marriage license and a marriage certificate ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>65089</td>\n",
       "      <td>gpt-3.5-turbo-0613</td>\n",
       "      <td>mistral-medium</td>\n",
       "      <td>explain function calling. how would you call a...</td>\n",
       "      <td>Function calling is the process of invoking or...</td>\n",
       "      <td>Function calling is the process of invoking a ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>96401</td>\n",
       "      <td>llama-2-13b-chat</td>\n",
       "      <td>mistral-7b-instruct</td>\n",
       "      <td>How can I create a test set for a very rare ca...</td>\n",
       "      <td>Creating a test set for a very rare category c...</td>\n",
       "      <td>When building a classifier for a very rare cat...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>198779</td>\n",
       "      <td>koala-13b</td>\n",
       "      <td>gpt-3.5-turbo-0314</td>\n",
       "      <td>What is the best way to travel from Tel-Aviv t...</td>\n",
       "      <td>The best way to travel from Tel Aviv to Jerusa...</td>\n",
       "      <td>The best way to travel from Tel-Aviv to Jerusa...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T14:36:02.525839Z",
     "start_time": "2025-08-14T14:36:02.348752Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train['text'] = 'User prompt: ' + train['prompt'] + '\\n\\nModel A :\\n' + train[\n",
    "    'response_a'] + '\\n\\n--------\\n\\nModel B:\\n' + train['response_b']\n",
    "print(train['text'][4])"
   ],
   "id": "4d40d17480c9be5f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User prompt: What is the best way to travel from Tel-Aviv to Jerusalem? Car? Bus? Plane?\n",
      "\n",
      "Model A :\n",
      "The best way to travel from Tel Aviv to Jerusalem depends on your personal preference and the availability of transportation options. All the options you have mentioned are valid options, but here are some details to help you make your decision:\\n\\n*   By car: Traveling by car is the quickest way to get from Tel Aviv to Jerusalem, as the distance between the two cities is only about 60 kilometers (37 miles). It takes around 45 minutes to drive from Tel Aviv to Jerusalem by car, depending on the traffic.\\n*   By bus: There are several bus lines that run from Tel Aviv to Jerusalem, and the journey takes around 1 hour and 30 minutes by bus. The buses are comfortable and reliable, and they offer a scenic view of the beautiful Israeli countryside.\\n*   By plane: There are no direct flights from Tel Aviv to Jerusalem, so you need to take a flight from Tel Aviv's Ben Gurion International Airport to Jerusalem's Sde Dov Airport. The flight takes around 30 minutes, and it is a convenient option if you are short on time.\\n\\nIn general, if you have enough time and don't mind taking the bus, I recommend that option. The buses are comfortable, reliable, and offer a scenic view of the countryside. If you are short on time and don't want to take a bus, a flight is another option to consider.\n",
      "\n",
      "--------\n",
      "\n",
      "Model B:\n",
      "The best way to travel from Tel-Aviv to Jerusalem depends on your personal preferences and circumstances. \\n\\nIf you have a car and enjoy driving, then driving from Tel-Aviv to Jerusalem is an option. However, parking in Jerusalem can be challenging and expensive.\\n\\nIf you prefer to use public transportation, there are several bus lines that operate between Tel-Aviv and Jerusalem. Some of the most popular bus companies include Egged and Dan. The bus ride typically takes about an hour, depending on traffic.\\n\\nTaking a plane is not a recommended option since Tel-Aviv and Jerusalem are relatively close cities, and there are no airports in Jerusalem. \\n\\nIn summary, taking a bus is the most commonly used and convenient way to travel from Tel-Aviv to Jerusalem.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T14:36:41.386179Z",
     "start_time": "2025-08-14T14:36:02.564493Z"
    }
   },
   "cell_type": "code",
   "source": "train.loc[:, 'token_count'] = get_token_lengths(train['text'])",
   "id": "b04672090cce8b57",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T14:36:41.438759Z",
     "start_time": "2025-08-14T14:36:41.430912Z"
    }
   },
   "cell_type": "code",
   "source": "train.head(5)",
   "id": "353361050108368f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       id             model_a              model_b  \\\n",
       "0   30192  gpt-4-1106-preview           gpt-4-0613   \n",
       "1   53567           koala-13b           gpt-4-0613   \n",
       "2   65089  gpt-3.5-turbo-0613       mistral-medium   \n",
       "3   96401    llama-2-13b-chat  mistral-7b-instruct   \n",
       "4  198779           koala-13b   gpt-3.5-turbo-0314   \n",
       "\n",
       "                                              prompt  \\\n",
       "0  Is it morally right to try to have a certain p...   \n",
       "1  What is the difference between marriage licens...   \n",
       "2  explain function calling. how would you call a...   \n",
       "3  How can I create a test set for a very rare ca...   \n",
       "4  What is the best way to travel from Tel-Aviv t...   \n",
       "\n",
       "                                          response_a  \\\n",
       "0  The question of whether it is morally right to...   \n",
       "1  A marriage license is a legal document that al...   \n",
       "2  Function calling is the process of invoking or...   \n",
       "3  Creating a test set for a very rare category c...   \n",
       "4  The best way to travel from Tel Aviv to Jerusa...   \n",
       "\n",
       "                                          response_b  winner_model_a  \\\n",
       "0  As an AI, I don't have personal beliefs or opi...               1   \n",
       "1  A marriage license and a marriage certificate ...               0   \n",
       "2  Function calling is the process of invoking a ...               0   \n",
       "3  When building a classifier for a very rare cat...               1   \n",
       "4  The best way to travel from Tel-Aviv to Jerusa...               0   \n",
       "\n",
       "   winner_model_b  winner_tie  \\\n",
       "0               0           0   \n",
       "1               1           0   \n",
       "2               0           1   \n",
       "3               0           0   \n",
       "4               1           0   \n",
       "\n",
       "                                                text  token_count  \n",
       "0  User prompt: Is it morally right to try to hav...         1205  \n",
       "1  User prompt: What is the difference between ma...         1392  \n",
       "2  User prompt: explain function calling. how wou...          663  \n",
       "3  User prompt: How can I create a test set for a...         1007  \n",
       "4  User prompt: What is the best way to travel fr...          478  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>model_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "      <th>winner_model_a</th>\n",
       "      <th>winner_model_b</th>\n",
       "      <th>winner_tie</th>\n",
       "      <th>text</th>\n",
       "      <th>token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30192</td>\n",
       "      <td>gpt-4-1106-preview</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>Is it morally right to try to have a certain p...</td>\n",
       "      <td>The question of whether it is morally right to...</td>\n",
       "      <td>As an AI, I don't have personal beliefs or opi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>User prompt: Is it morally right to try to hav...</td>\n",
       "      <td>1205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53567</td>\n",
       "      <td>koala-13b</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>What is the difference between marriage licens...</td>\n",
       "      <td>A marriage license is a legal document that al...</td>\n",
       "      <td>A marriage license and a marriage certificate ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>User prompt: What is the difference between ma...</td>\n",
       "      <td>1392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>65089</td>\n",
       "      <td>gpt-3.5-turbo-0613</td>\n",
       "      <td>mistral-medium</td>\n",
       "      <td>explain function calling. how would you call a...</td>\n",
       "      <td>Function calling is the process of invoking or...</td>\n",
       "      <td>Function calling is the process of invoking a ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>User prompt: explain function calling. how wou...</td>\n",
       "      <td>663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>96401</td>\n",
       "      <td>llama-2-13b-chat</td>\n",
       "      <td>mistral-7b-instruct</td>\n",
       "      <td>How can I create a test set for a very rare ca...</td>\n",
       "      <td>Creating a test set for a very rare category c...</td>\n",
       "      <td>When building a classifier for a very rare cat...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>User prompt: How can I create a test set for a...</td>\n",
       "      <td>1007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>198779</td>\n",
       "      <td>koala-13b</td>\n",
       "      <td>gpt-3.5-turbo-0314</td>\n",
       "      <td>What is the best way to travel from Tel-Aviv t...</td>\n",
       "      <td>The best way to travel from Tel Aviv to Jerusa...</td>\n",
       "      <td>The best way to travel from Tel-Aviv to Jerusa...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>User prompt: What is the best way to travel fr...</td>\n",
       "      <td>478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T14:36:41.498711Z",
     "start_time": "2025-08-14T14:36:41.487676Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train.loc[:, 'label'] = np.argmax(train[['winner_model_a', 'winner_model_b', 'winner_tie']].values, axis=1)\n",
    "\n",
    "# Display data\n",
    "display(train.head())"
   ],
   "id": "2fffc64e5524865e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       id             model_a              model_b  \\\n",
       "0   30192  gpt-4-1106-preview           gpt-4-0613   \n",
       "1   53567           koala-13b           gpt-4-0613   \n",
       "2   65089  gpt-3.5-turbo-0613       mistral-medium   \n",
       "3   96401    llama-2-13b-chat  mistral-7b-instruct   \n",
       "4  198779           koala-13b   gpt-3.5-turbo-0314   \n",
       "\n",
       "                                              prompt  \\\n",
       "0  Is it morally right to try to have a certain p...   \n",
       "1  What is the difference between marriage licens...   \n",
       "2  explain function calling. how would you call a...   \n",
       "3  How can I create a test set for a very rare ca...   \n",
       "4  What is the best way to travel from Tel-Aviv t...   \n",
       "\n",
       "                                          response_a  \\\n",
       "0  The question of whether it is morally right to...   \n",
       "1  A marriage license is a legal document that al...   \n",
       "2  Function calling is the process of invoking or...   \n",
       "3  Creating a test set for a very rare category c...   \n",
       "4  The best way to travel from Tel Aviv to Jerusa...   \n",
       "\n",
       "                                          response_b  winner_model_a  \\\n",
       "0  As an AI, I don't have personal beliefs or opi...               1   \n",
       "1  A marriage license and a marriage certificate ...               0   \n",
       "2  Function calling is the process of invoking a ...               0   \n",
       "3  When building a classifier for a very rare cat...               1   \n",
       "4  The best way to travel from Tel-Aviv to Jerusa...               0   \n",
       "\n",
       "   winner_model_b  winner_tie  \\\n",
       "0               0           0   \n",
       "1               1           0   \n",
       "2               0           1   \n",
       "3               0           0   \n",
       "4               1           0   \n",
       "\n",
       "                                                text  token_count  label  \n",
       "0  User prompt: Is it morally right to try to hav...         1205      0  \n",
       "1  User prompt: What is the difference between ma...         1392      1  \n",
       "2  User prompt: explain function calling. how wou...          663      2  \n",
       "3  User prompt: How can I create a test set for a...         1007      0  \n",
       "4  User prompt: What is the best way to travel fr...          478      1  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>model_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "      <th>winner_model_a</th>\n",
       "      <th>winner_model_b</th>\n",
       "      <th>winner_tie</th>\n",
       "      <th>text</th>\n",
       "      <th>token_count</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30192</td>\n",
       "      <td>gpt-4-1106-preview</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>Is it morally right to try to have a certain p...</td>\n",
       "      <td>The question of whether it is morally right to...</td>\n",
       "      <td>As an AI, I don't have personal beliefs or opi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>User prompt: Is it morally right to try to hav...</td>\n",
       "      <td>1205</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53567</td>\n",
       "      <td>koala-13b</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>What is the difference between marriage licens...</td>\n",
       "      <td>A marriage license is a legal document that al...</td>\n",
       "      <td>A marriage license and a marriage certificate ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>User prompt: What is the difference between ma...</td>\n",
       "      <td>1392</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>65089</td>\n",
       "      <td>gpt-3.5-turbo-0613</td>\n",
       "      <td>mistral-medium</td>\n",
       "      <td>explain function calling. how would you call a...</td>\n",
       "      <td>Function calling is the process of invoking or...</td>\n",
       "      <td>Function calling is the process of invoking a ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>User prompt: explain function calling. how wou...</td>\n",
       "      <td>663</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>96401</td>\n",
       "      <td>llama-2-13b-chat</td>\n",
       "      <td>mistral-7b-instruct</td>\n",
       "      <td>How can I create a test set for a very rare ca...</td>\n",
       "      <td>Creating a test set for a very rare category c...</td>\n",
       "      <td>When building a classifier for a very rare cat...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>User prompt: How can I create a test set for a...</td>\n",
       "      <td>1007</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>198779</td>\n",
       "      <td>koala-13b</td>\n",
       "      <td>gpt-3.5-turbo-0314</td>\n",
       "      <td>What is the best way to travel from Tel-Aviv t...</td>\n",
       "      <td>The best way to travel from Tel Aviv to Jerusa...</td>\n",
       "      <td>The best way to travel from Tel-Aviv to Jerusa...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>User prompt: What is the best way to travel fr...</td>\n",
       "      <td>478</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T14:36:41.570742Z",
     "start_time": "2025-08-14T14:36:41.564457Z"
    }
   },
   "cell_type": "code",
   "source": "train.label.value_counts()",
   "id": "fb2a0e86006373ad",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    20061\n",
       "1    19648\n",
       "2    17749\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T14:36:41.677672Z",
     "start_time": "2025-08-14T14:36:41.667410Z"
    }
   },
   "cell_type": "code",
   "source": "train['token_count'].describe().to_frame().astype(int)",
   "id": "937cd49e2872b211",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       token_count\n",
       "count        57458\n",
       "mean           731\n",
       "std            790\n",
       "min             16\n",
       "25%            290\n",
       "50%            565\n",
       "75%            894\n",
       "max          31034"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>57458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>31034</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T14:36:41.789871Z",
     "start_time": "2025-08-14T14:36:41.784639Z"
    }
   },
   "cell_type": "code",
   "source": "np.percentile(train['token_count'], 90)",
   "id": "14c6bf11ecb11c14",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1396.0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T14:37:16.145012Z",
     "start_time": "2025-08-14T14:36:41.875162Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokens = tokenizer(\n",
    "    train['text'].tolist(),\n",
    "    padding='max_length',\n",
    "    max_length=CFG.MAX_LENGTH,\n",
    "    truncation=True,\n",
    "    return_tensors='np',\n",
    "\n",
    ")\n",
    "\n",
    "# input IDs are the token IDs\n",
    "INPUT_IDS = tokens['input_ids']\n",
    "# Attention masks to Ignore padding tokens\n",
    "ATTENTION_MASKS = tokens['attention_mask']\n",
    "\n",
    "# Label of texts\n",
    "LABELS = train[['winner_model_a', 'winner_model_b', 'winner_tie']].values\n",
    "\n",
    "print(f'INPUT_IDS shape: {INPUT_IDS.shape}, ATTENTION_MASKS shape: {ATTENTION_MASKS.shape}')\n",
    "print(f'LABELS shape: {LABELS.shape}')"
   ],
   "id": "c163ba0953575d16",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT_IDS shape: (57458, 1024), ATTENTION_MASKS shape: (57458, 1024)\n",
      "LABELS shape: (57458, 3)\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T14:57:45.221441Z",
     "start_time": "2025-08-14T14:57:45.217163Z"
    }
   },
   "cell_type": "code",
   "source": "LABELS[:3]",
   "id": "c474170f8e80ee94",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T14:38:09.985385Z",
     "start_time": "2025-08-14T14:38:09.980863Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_dataset(batch_size):\n",
    "    N_SAMPLES = LABELS.shape[0]\n",
    "    IDXS = np.arange(N_SAMPLES - (N_SAMPLES % batch_size))\n",
    "    while True:\n",
    "        np.random.shuffle(IDXS)\n",
    "\n",
    "        for idxs in IDXS.reshape(-1, batch_size):\n",
    "            input_ids = torch.tensor(INPUT_IDS[idxs]).to(device)\n",
    "            attention_mask = torch.tensor(ATTENTION_MASKS[idxs]).to(device)\n",
    "            labels = torch.tensor(LABELS[idxs]).to(device)\n",
    "\n",
    "            yield input_ids, attention_mask, labels\n",
    "\n",
    "\n",
    "TRAIN_DATASET = train_dataset(CFG.BATCH_SIZE)"
   ],
   "id": "b5ea92ec5acf92ca",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T15:07:56.165557Z",
     "start_time": "2025-08-14T15:07:56.091782Z"
    }
   },
   "cell_type": "code",
   "source": [
    "base_model = LlamaForSequenceClassification.from_pretrained(\n",
    "    llama3_path,\n",
    "    num_labels=CFG.NUM_LABELS,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "base_model.config.pretraining_tp = 1\n",
    "\n",
    "base_model.config.pad_token_id = tokenizer.pad_token_id"
   ],
   "id": "d842b6ae84e336e7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0ff158c255a84df9896b158b54af24ed"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at C:\\Users\\Administrator\\.cache\\kagglehub\\models\\metaresearch\\llama-3\\transformers\\8b-hf\\1 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T14:38:14.678364Z",
     "start_time": "2025-08-14T14:38:14.675012Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=CFG.LORA_RANK,\n",
    "    lora_alpha=CFG.LORA_ALPHA,\n",
    "    lora_dropout=CFG.DROPOUT,\n",
    "    bias='none',\n",
    "    inference_mode=False,\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    target_modules=CFG.LORA_MODULES\n",
    ")"
   ],
   "id": "de52440123f896ec",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T14:38:25.832500Z",
     "start_time": "2025-08-14T14:38:16.824662Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = get_peft_model(base_model, lora_config).to(device)\n",
    "model.print_trainable_parameters()"
   ],
   "id": "d98df038e252a5e4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6,828,032 || all params: 7,511,764,992 || trainable%: 0.0909\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T14:38:38.686809Z",
     "start_time": "2025-08-14T14:38:38.673686Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MODEL_LAYERS_ROWS = []\n",
    "TRAINABLE_PARAMS = []\n",
    "N_TRAINABLE_PARAMS = 0\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    n_parameters = int(torch.prod(torch.tensor(param.shape)))\n",
    "\n",
    "    if param.requires_grad:\n",
    "        MODEL_LAYERS_ROWS.append({\n",
    "            'param': n_parameters,\n",
    "            'name': name,\n",
    "            'dtype': param.data.dtype,\n",
    "        })\n",
    "\n",
    "        TRAINABLE_PARAMS.append({'param': param})\n",
    "\n",
    "        N_TRAINABLE_PARAMS += n_parameters\n",
    "display(pd.DataFrame(MODEL_LAYERS_ROWS))\n",
    "\n",
    "print(f\"\"\"\n",
    "===============================\n",
    "N_TRAINABLE_PARAMS: {N_TRAINABLE_PARAMS:,}\n",
    "N_TRAINABLE_LAYERS: {len(TRAINABLE_PARAMS)}\n",
    "===============================\n",
    "\"\"\")"
   ],
   "id": "f6f217c0b7e87949",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     param                                               name           dtype\n",
       "0    65536  base_model.model.model.layers.0.self_attn.q_pr...   torch.float32\n",
       "1    65536  base_model.model.model.layers.0.self_attn.q_pr...   torch.float32\n",
       "2    65536  base_model.model.model.layers.0.self_attn.v_pr...   torch.float32\n",
       "3    16384  base_model.model.model.layers.0.self_attn.v_pr...   torch.float32\n",
       "4    65536  base_model.model.model.layers.1.self_attn.q_pr...   torch.float32\n",
       "..     ...                                                ...             ...\n",
       "124  65536  base_model.model.model.layers.31.self_attn.q_p...   torch.float32\n",
       "125  65536  base_model.model.model.layers.31.self_attn.q_p...   torch.float32\n",
       "126  65536  base_model.model.model.layers.31.self_attn.v_p...   torch.float32\n",
       "127  16384  base_model.model.model.layers.31.self_attn.v_p...   torch.float32\n",
       "128  12288  base_model.model.score.modules_to_save.default...  torch.bfloat16\n",
       "\n",
       "[129 rows x 3 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param</th>\n",
       "      <th>name</th>\n",
       "      <th>dtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>65536</td>\n",
       "      <td>base_model.model.model.layers.0.self_attn.q_pr...</td>\n",
       "      <td>torch.float32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>65536</td>\n",
       "      <td>base_model.model.model.layers.0.self_attn.q_pr...</td>\n",
       "      <td>torch.float32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>65536</td>\n",
       "      <td>base_model.model.model.layers.0.self_attn.v_pr...</td>\n",
       "      <td>torch.float32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16384</td>\n",
       "      <td>base_model.model.model.layers.0.self_attn.v_pr...</td>\n",
       "      <td>torch.float32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>65536</td>\n",
       "      <td>base_model.model.model.layers.1.self_attn.q_pr...</td>\n",
       "      <td>torch.float32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>65536</td>\n",
       "      <td>base_model.model.model.layers.31.self_attn.q_p...</td>\n",
       "      <td>torch.float32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>65536</td>\n",
       "      <td>base_model.model.model.layers.31.self_attn.q_p...</td>\n",
       "      <td>torch.float32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>65536</td>\n",
       "      <td>base_model.model.model.layers.31.self_attn.v_p...</td>\n",
       "      <td>torch.float32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>16384</td>\n",
       "      <td>base_model.model.model.layers.31.self_attn.v_p...</td>\n",
       "      <td>torch.float32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>12288</td>\n",
       "      <td>base_model.model.score.modules_to_save.default...</td>\n",
       "      <td>torch.bfloat16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>129 rows × 3 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===============================\n",
      "N_TRAINABLE_PARAMS: 6,828,032\n",
      "N_TRAINABLE_LAYERS: 129\n",
      "===============================\n",
      "\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T14:38:44.633647Z",
     "start_time": "2025-08-14T14:38:44.619522Z"
    }
   },
   "cell_type": "code",
   "source": [
    "N_SAMPLES = len(train)\n",
    "STEPS_PER_EPOCH = N_SAMPLES // CFG.BATCH_SIZE\n",
    "\n",
    "OPTIMIZER = torch.optim.AdamW(model.parameters(), lr=CFG.LR_MAX)\n",
    "\n",
    "# Cosine Learning Rate With Warmup\n",
    "lr_scheduler = transformers.get_cosine_schedule_with_warmup(\n",
    "    optimizer=OPTIMIZER,\n",
    "    num_warmup_steps=CFG.NUM_WARMUP_STEPS,\n",
    "    num_training_steps=STEPS_PER_EPOCH * CFG.NUM_EPOCH\n",
    ")\n",
    "print(f'BATCH_SIZE: {CFG.BATCH_SIZE}, N_SAMPLES: {N_SAMPLES}, STEPS_PER_EPOCH: {STEPS_PER_EPOCH}')"
   ],
   "id": "7b712a5e42403ec1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH_SIZE: 16, N_SAMPLES: 57458, STEPS_PER_EPOCH: 3591\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T14:38:48.768231Z",
     "start_time": "2025-08-14T14:38:48.759145Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for state in OPTIMIZER.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor) and state[k].dtype is not torch.float32:\n",
    "            state[k] = v.to(dtype=torch.float32)\n",
    "\n",
    "input_ids, attention_mask, labels = next(TRAIN_DATASET)\n",
    "\n",
    "print(f'input_ids shape: {input_ids.shape}, dtype: {input_ids.dtype}')\n",
    "print(f'attention_mask shape: {attention_mask.shape}, dtype: {attention_mask.dtype}')\n",
    "print(f'labels shape: {labels.shape}, dtype: {labels.dtype}')"
   ],
   "id": "f62bf7d2cca5f5db",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape: torch.Size([16, 1024]), dtype: torch.int64\n",
      "attention_mask shape: torch.Size([16, 1024]), dtype: torch.int64\n",
      "labels shape: torch.Size([16, 3]), dtype: torch.int64\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T14:42:50.169316Z",
     "start_time": "2025-08-14T14:39:14.052135Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%time\n",
    "# Dummy Prediction\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    outputs = model(input_ids=input_ids.to(device), attention_mask=attention_mask.to(device))\n",
    "print(f'logits: {outputs.logits}, dtype: {outputs.logits.dtype}')"
   ],
   "id": "6d3c639d467b2584",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits: tensor([[-1.9141,  2.1094,  0.4961],\n",
      "        [ 1.1484,  2.7656,  3.1250],\n",
      "        [ 0.6953,  2.7812,  1.8750],\n",
      "        [-2.6719,  3.7500,  0.3750],\n",
      "        [ 2.4375,  4.1250,  0.9844],\n",
      "        [-0.8125,  0.2119,  0.5273],\n",
      "        [ 0.6719,  2.0469,  3.0000],\n",
      "        [-0.1074,  3.4688,  0.8438],\n",
      "        [-1.1719, -0.1367, -3.2344],\n",
      "        [ 1.1406,  4.3438,  3.6250],\n",
      "        [-0.9766,  5.6562, -1.2734],\n",
      "        [-2.1562,  1.1250,  0.8281],\n",
      "        [ 0.3828, -3.2188,  2.7500],\n",
      "        [-0.9922, -1.1250, -4.7188],\n",
      "        [ 2.5781, -0.5781, -0.5859],\n",
      "        [ 0.0547,  6.3750, -0.4531]], device='cuda:0', dtype=torch.bfloat16), dtype: torch.bfloat16\n",
      "CPU times: total: 3min 33s\n",
      "Wall time: 3min 36s\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T14:52:26.083767Z",
     "start_time": "2025-08-14T14:52:26.066125Z"
    }
   },
   "cell_type": "code",
   "source": [
    "logits = outputs.logits.cpu().to(dtype=torch.float32)\n",
    "label = torch.argmax(F.softmax(logits, dim=-1), dim=-1).cpu().tolist()\n",
    "for i in range(input_ids.shape[0]):\n",
    "    # Get the input_ids for a single example\n",
    "    single_input_ids = input_ids[i].cpu().tolist()\n",
    "\n",
    "    # Decode the single example\n",
    "    decoded_string = tokenizer.decode(single_input_ids, skip_special_tokens=True)\n",
    "\n",
    "    # Now you can print or process the decoded string for this example\n",
    "    print(f\"Decoded string for example {i}: {decoded_string} \\n \\n label: {label[i]}\")"
   ],
   "id": "fe84bf7c5c8bb6ef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded string for example 0: User prompt: I need a list of tips to help my 3 year old toddler eat food, he's currently not eating anywhere near enough\n",
      "\n",
      "Model A :\n",
      "Here are some tips to help your 3 year old toddler eat food:\\n1. Offer a variety of nutritious and appealing foods.\\n2. Allow plenty of time for meals and snacks.\\n3. Set a good example by eating healthy yourself.\\n4. Introduce new foods one at a time.\\n5. Serve small portions and let your toddler ask for more.\\n6. Provide healthy snacks and meals when your toddler is hungry.\\n7. Include finger foods and other easy-to-manage foods.\\n8. Include your toddler in meal and snack preparation.\\n9. Make meal and snack times positive and relaxed.\\n10. Use visual cues such as small plates and bowls to help your toddler keep track of the amount of food they are eating.\n",
      "\n",
      "--------\n",
      "\n",
      "Model B:\n",
      "Here are some tips that might help your 3-year-old toddler eat more:\\n\\n1. Offer a variety of healthy foods: Offer your child a variety of healthy foods such as fruits, vegetables, whole grains, lean proteins, and dairy products. Try to include at least one protein and one fruit or vegetable in each meal.\\n\\n2. Serve at the right temperature: Make sure your meals are served at the right temperature. You can use a thermometer to check the temperature of the food. If the temperature is too hot, your child may not enjoy it. If it's too cold, it may not be safe to eat.\\n\\n3. Let your child help prepare food: Let your child help you prepare meals. This will give them a sense of responsibility and help them learn how to make healthy choices.\\n\\n4. Encourage eating at the table: Encourage your child to eat at the table by setting a good example. Eating together as a family can help build mealtime routines and create a positive atmosphere.\\n\\n5. Offer healthy snacks: Snacks can be a great way to provide your child with energy and help them maintain a healthy weight. Offer healthy snacks such as fruits and vegetables, whole grains, and nuts.\\n\\n6. Model healthy eating behaviors: Model healthy eating behaviors by eating a healthy diet yourself and by promoting healthy food choices in your home.\\n\\nRemember, it's important to be patient and persistent in your efforts to help your child eat more. With time and effort, you may be able to help him develop a healthy eating habit. \n",
      " \n",
      " label: 1\n",
      "Decoded string for example 1: User prompt: You are an expert programmer that can write programs to answer natural language questions using functions.  \\nYour task is to write a python program that calls functions to get information and combine them to answer a question. \\n\\nYou will be given the following functions:\\nfact_extractor: \\nThis function takes as input valid wikidata PID and a subject QID: fact_extractor(pid=PID, subject_qid=QID),\\nand it retrieves the subject literal and the subject QID.\\nDo not to write the fact_extractor function yourself, assume it is given!\\nYou need to pass QIDs and PIDs to the fact_extractor function.\\nYou will be given QIDs for the entities in the question, \\nplease insert them in the appropriate position in the fact_extractor function.\\nYou know the required property IDs (PIDs) and you need insert them where appropriate in the fact_extractor function.\\nSometimes you will be asked the same question again, in this case generate the same one program as you have generated before.\\nAlways generate a program and only a single one not several, even if you have writen such a program in the past.\\n\\nYou will be given the following input:\\nQuestion: the input question you must answer\\nEntities and QIDs: you will be given QIDs of the entities in the question\\n\\nUse the following output format:\\nThought: you should always think about what to do\\nPython program: the program that you write to call the given functions to answer the question, \\nthe program should print out an answer sentence including the inserted answer string. \\n\\nQuestion: When was Michael Jackson born?\\nEntities and QIDs:\\n    Michael Jackson: Q2831\\nThought:\n",
      "\n",
      "Model A :\n",
      "I need to find the birth date of Michael Jackson by calling the fact_extractor function with the appropriate PID and QID. Since I don't have the fact_extractor function implementation, I'll assume it can extract the birth date (PID) from the entity (QID).\\n\\nPython program:\\n```python\\nfact_extractor(pid=PID_BIRTH_DATE, subject_qid=Q2831)\\n```\\nNote: Replace `PID_BIRTH_DATE` with the actual property ID for the birth date in the Wikidata dataset.\n",
      "\n",
      "--------\n",
      "\n",
      "Model B:\n",
      "I need to find the birth date of Michael Jackson using the fact_extractor function. I know the QID for Michael Jackson is Q2831, and the PID for date of birth is P569. I will write a program that calls the fact_extractor function with these parameters and prints the birth date in a sentence. \n",
      " \n",
      " label: 2\n",
      "Decoded string for example 2: User prompt: what are you capable of\n",
      "\n",
      "Model A :\n",
      "I am capable of understanding and generating text based on the input I receive, answering questions, providing information, and engaging in various types of conversations. I can process and synthesize large amounts of information from a wide range of topics, and can also generate creative content such as stories, poems, and jokes. Additionally, I can assist with problem-solving, language translation, and even programming tasks. However, my capabilities are limited to the knowledge and algorithms I have been trained on, and I may not always provide accurate or complete information.\n",
      "\n",
      "--------\n",
      "\n",
      "Model B:\n",
      "I am an artificial intelligence assistant capable of processing and understanding natural language queries to provide helpful and informative responses. I can assist with a wide range of topics including general knowledge, language translation, problem-solving, and more. I strive to provide accurate and relevant information to the best of my abilities. How can I assist you today? \n",
      " \n",
      " label: 1\n",
      "Decoded string for example 3: User prompt: We have two different things: the video game Silent Hill and the video game Legend of Zelda: Breath of the Wild. In 7 Steps try to find videogames or films that are less and less similar to the first thing and more and more similar to the second thing. Don't do anything else besides the list of actual video games or films and their bullet description.\n",
      "\n",
      "Model A :\n",
      "1. Resident Evil 2 (2019) - A survival horror game that features a mysterious, abandoned town filled with terrifying creatures.\\n2. Until Dawn (2015) - A horror game that follows a group of teenagers on a ski trip who find themselves pursued by a killer.\\n3. The Last of Us (2013) - A post-apocalyptic survival game that focuses on a young girl and a woman fighting to survive against hordes of infected.\\n4. Bloodborne (2015) - A dark, gothic action RPG set in a haunted city filled with terrifying creatures.\\n5. Dark Souls (2011) - A punishing action RPG that requires careful exploration and strategic combat to survive.\\n6. Demon's Souls (2009) - A challenging action RPG that requires players to explore a dark, dangerous world filled with terrifying enemies.\\n7. Shadow of the Colossus (2018) - A puzzle-adventure game that follows a young man on a quest to restore the life of a young woman by slaying giant creatures.\n",
      "\n",
      "--------\n",
      "\n",
      "Model B:\n",
      "1. Silent Hill (Video Game) - A psychological survival horror game set in a foggy, eerie town filled with nightmarish creatures, puzzles, and a deep narrative.\\n\\n2. Alan Wake (Video Game) - A psychological action thriller game with a writer protagonist, supernatural elements, and an eerie atmosphere, but with more focus on action and exploration.\\n\\n3. The Last Guardian (Video Game) - A puzzle-platformer with a focus on exploration and a close bond between the protagonist and a mysterious creature, set in a beautiful yet haunting world.\\n\\n4. Shadow of the Colossus (Video Game) - An action-adventure game with vast landscapes, exploration, and unique boss battles, featuring a protagonist trying to save a loved one.\\n\\n5. The Elder Scrolls V: Skyrim (Video Game) - An open-world action RPG with a large explorable world, various quests, and a focus on character customization, but with a more medieval fantasy setting.\\n\\n6. The Witcher 3: Wild Hunt (Video Game) - A third-person action RPG with a vast open-world filled with quests, exploration, and monster hunting, set in a high fantasy world with a complex narrative.\\n\\n7. The Legend of Zelda: Breath of the Wild (Video Game) - An open-world action-adventure game with a focus on exploration, puzzle-solving, and combat in a beautifully crafted high-fantasy world, featuring a heroic protagonist on a quest to save the kingdom. \n",
      " \n",
      " label: 1\n",
      "Decoded string for example 4: User prompt: Suggest just one  Conventional Commits message based on the following diff. \\nThe commit message should have description with a short title that follows Conventional Commits message format like <type>[scope]: <description>.\\nIf it's a small change, show the target of the change as well. use abbreviation when possilbe, eg. env\\/dev\\/prod\\/deploy\\n\\n\\nExamples:\\n- fix(authentication): add 403 forbidden error cases\\n- feat(ai): generate commit messages from changes\\n\\nIf necessary, follow with an explanatory body providing insight into the nature of the changes, the reasoning behind them, and any significant consequences or considerations arising from them.\\n\\nit should start with a high level summary then followed by more details. Keep it short and informative use bullet points when possible.\\u00a0\\n\\nDiff:\\u00a0\\n\\u00a0\\u00a0\\u00a0private static Logger logger = LoggerFactory.getLogger(ApplicationStartup.class);\\n\\u00a0\\u00a0\\u00a0private ForkJoinPool forkJoinPool;\\n\\n\\u00a0\\u00a0\\u00a0private List<MessageConsumer> messageConsumers = new ArrayList<>();\\n\\n\\u00a0\\u00a0\\u00a0@Autowired\\n\\u00a0\\u00a0\\u00a0private ApplicationContext applicationContext;\\n\\n\\u00a0\\u00a0\\u00a0@PostConstruct\\n\\u00a0\\u00a0\\u00a0private void init() {\\n\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0int parallelism = Runtime.getRuntime().availableProcessors() - 1;\\n\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0if (parallelism <= 10) {\\n\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0parallelism = 10;\\n\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0}\\n\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0forkJoinPool = new ForkJoinPool(parallelism, ForkJoinPool.defaultForkJoinWorkerThreadFactory, null, true);\\n\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0for (int i = 0; i < parallelism \\/ 3; i++) {\\n\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0PreHandlingMessageConsumer messageConsumer = applicationContext.getBean(PreHandlingMessageConsumer.class);\\n\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0messageConsumers.add(messageConsumer);\\n\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0}\\n\\n-\\u00a0\\u00a0\\u00a0\\u00a0for (int i = 0; i < parallelism; i++) {\\n-\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0DeletionMessageConsumer deletionMessageConsumer = applicationContext.getBean(DeletionMessageConsumer.class);\\n-\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0messageConsumers.add(deletionMessageConsumer);\\n-\\u00a0\\u00a0\\u00a0\\u00a0}\\n\\n\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0FeedbackAlertConsumer feedbackAlertConsumer = applicationContext.getBean(FeedbackAlertConsumer.class);\\n\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0messageConsumers.add(feedbackAlertConsumer);\\n\\n\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0for (int i = 0; i < parallelism ; i++) {\\n\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0GdprDeleteUserConsumer gdprDeleteUserConsumer = applicationContext.getBean(GdprDeleteUserConsumer.class);\\n\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0messageConsumers.add(gdprDeleteUserConsumer);\\n\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0}\\n\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0logger.info(\\\"The movie maker has initialized successfully with parallelism {}.\\\", parallelism);\\ \n",
      " \n",
      " label: 1\n",
      "Decoded string for example 5: User prompt: do you speak spanish?\n",
      "\n",
      "Model A :\n",
      "Yes, I can speak Spanish. How can I help you with it?\n",
      "\n",
      "--------\n",
      "\n",
      "Model B:\n",
      "I'm sorry to say, but I do not speak any language in the traditional sense. I am a computer program designed to help answer questions and provide information based on the patterns and data it has been trained on. I do not have the ability to speak or communicate in a traditional sense. However, I can provide written responses in several languages, including Spanish, based on the input provided by users. \n",
      " \n",
      " label: 2\n",
      "Decoded string for example 6: User prompt: who is to blame?\n",
      "\n",
      "Model A :\n",
      "It's not clear who is to blame for the current situation.\n",
      "\n",
      "--------\n",
      "\n",
      "Model B:\n",
      "I do not have enough context to determine who is to blame for what. As an AI assistant without direct knowledge of the situation, I cannot make judgments about blameworthiness. I'd suggest providing more details about the specific incident or issue in question. \n",
      " \n",
      " label: 2\n",
      "Decoded string for example 7: User prompt: What is targetted sandbagging in the context of AI LLM models and why is it important in AI alignment?\n",
      "\n",
      "Model A :\n",
      "The term \\\"targeted sandbagging\\\" in the context of AI, particularly large language models (LLMs), is not a standard term and does not have a widely agreed-upon definition in the field of AI research or AI alignment. However, I can infer a possible meaning based on the individual words.\\n\\nIn general, \\\"sandbagging\\\" refers to the practice of deliberately underperforming or holding back one's abilities or potential. For example, in games or sports, a player might sandbag by not playing to their full potential in order to deceive opponents about their true skills or to manipulate the difficulty of future matches.\\n\\nWhen we talk about \\\"targeted sandbagging\\\" in AI, it could conceptually refer to an AI intentionally limiting its abilities or outputs in specific areas or tasks. This could be implemented for various reasons, such as to prevent the model from revealing sensitive information, to avoid ethical concerns, or to maintain alignment with human values by not overstepping boundaries set by its developers.\\n\\nIn the context of AI alignment, which is the field concerned with ensuring that AI systems' goals and behaviors are aligned with human values and intentions, targeted sandbagging could be seen as a safeguard. It might be a strategic way to prevent an AI from acting in ways that could be harmful or unintended by its creators. By intentionally restricting the AI's performance in areas that could lead to misalignment, developers could reduce the risk of undesirable outcomes.\\n\\nFor example, an AI language model could be programmed to avoid generating content in areas where it may inadvertently create false information, cause harm, or violate ethical guidelines. This could be seen as a form of targeted sandbagging, where the model is \\\"holding back\\\" its capabilities in specific, targeted areas to maintain alignment with human intentions and ethical standards.\\n\\nIt is worth noting that while such strategies could be useful, they might also limit the usefulness and capabilities of the AI in certain contexts. Therefore, the concept of targeted sandbagging, if applied, would need to be balanced with the goals and purposes of the AI system.\\n\\nSince \\\"targeted sandbagging\\\" is not a standard term in AI, if you have encountered this term in a specific document or conversation, I recommend referring to that source for a more precise definition or explanation in the context in which it was used.\n",
      "\n",
      "--------\n",
      "\n",
      "Model B:\n",
      "Targeted sandbagging refers to a technique used in the context of AI LLM (Large Language Model) models, where the models are deliberately trained to perform below their actual capabilities in order to mitigate potential risks. Sandbagging is important in AI alignment because it helps address concerns related to the misuse or unintended consequences of AI systems.\\n\\nBy intentionally training AI models to underperform, researchers aim to reduce the risk of the models being used for malicious purposes or causing harm unintentionally. This technique allows for the exploration of potential risks and vulnerabilities associated with AI systems, and helps in understanding and addressing AI alignment challenges.\\n\\nTargeted sandbagging can help in ensuring that AI models are aligned with human values and goals, and that they make decisions that are safe, ethical, and beneficial for society. It allows researchers and developers to proactively identify and mitigate potential risks, fostering responsible AI development and deployment. \n",
      " \n",
      " label: 1\n",
      "Decoded string for example 8: User prompt: What is 5+5-1\n",
      "\n",
      "Model A :\n",
      "It is equal to 4\n",
      "\n",
      "--------\n",
      "\n",
      "Model B:\n",
      "5+5-1 = 8 \n",
      " \n",
      " label: 1\n",
      "Decoded string for example 9: User prompt: Can you write a story about a young named Emily? She goes to the pool with her friends. When she gets there, she has to pee. While undressing, she squats to take a sly pee on the floor. Her hissing pee and the gurgle of the floor drain give her away to her friends though, who call her out for it and find it very funny.\n",
      "\n",
      "Model A :\n",
      "Once upon a time, in a small town named Serenity Springs, lived a young girl named Emily. Emily was a lively, adventurous girl, always up for a good time with her friends. One sunny afternoon, Emily and her friends decided to spend the day at the local pool, hoping to beat the heat and have some fun.\\n\\nThe pool was a popular spot, bustling with people of all ages. Emily and her friends, Laura, Sarah, and Ben, settled into their favorite spot under a large umbrella, ready to enjoy the day. Emily, excited to take a dip, quickly changed into her swimsuit, a vibrant blue two-piece with white flowers.\\n\\nHowever, as Emily was about to take a plunge into the refreshing water, she felt a sudden and unexpected urge - she needed to pee. The public restrooms were quite far from their spot, and the line was long, which made Emily feel a bit anxious. She didn't want to leave her belongings unguarded and have to wait in the long line.\\n\\nIn a moment of mischief, Emily decided to take a sly pee on the floor. She squatted behind a large potted plant, hoping to remain unnoticed. Her friends were too engrossed in their own games and conversations to notice her absence. Emily's heart raced as she tried to be as quick and quiet as possible.\\n\\nNevertheless, Emily's furtive action was accompanied by two distinct sounds that she hadn't anticipated. The hissing of her pee as it hit the floor and the gurgle of the nearby floor drain echoed in the otherwise quiet corner she had chosen. Her friends, Laura, Sarah, and Ben, raised their heads, their eyes scanning the area, their ears having picked up the unusual sounds.\\n\\n\\\"What's that sound?\\\" Laura asked, her eyes squinting as she looked around.\\n\\nSarah chuckled, \\\"Sounds like someone's taking a pee!\\\"\\n\\nEmily's face flushed with embarrassment as her friends discovered her hiding behind the plant. They burst into fits of laughter, finding the whole situation quite amusing.\\n\\n\\\"Emily, you sneaky thing!\\\" Ben exclaimed, holding his stomach as he laughed.\\n\\nEmily, feeling embarrassed at first, eventually couldn't help but join in their laughter. She realized that her friends were not laughing at her, but with her, enjoying the shared moment of mischief. The incident added a funny anecdote to their memories of the day, a story they would retell, laugh about, and cherish for years to come.\\n\\nThe rest of the day passed by with laughter and splashing in the pool, making memories under the sun. Emily learned that sometimes, even in the most unexpected and embarrassing situations, you can find a moment of shared joy with your friends. And even though the incident was a little awkward, it became a cherished memory, a testament to their friendship and the unforgettable summer days at the pool.\n",
      "\n",
      "--------\n",
      "\n",
      "Model B:\n",
      "Emily was a young girl with a vibrant personality and a love for adventure. She was always up for a good time and enjoyed spending her days with her friends, exploring the world around her. On this particular day, she and her friends had decided to go to the local pool for a fun-filled afternoon of swimming and sunbathing.\\n\\nAs they arrived at the pool, Emily suddenly realized that she had to pee. She looked around for the bathroom, but it was too far away and she didn't want to miss out on the fun. So, she decided to take a sly pee on the floor, hoping that no one would notice.\\n\\nEmily quickly undressed and squatted down near the edge of the pool. She let out a hissing pee, the sound echoing in the quiet of the afternoon. To her horror, the gurgle of the floor drain gave her away and her friends turned to look at her, laughing and calling her out for her mischievous act.\\n\\nEmily was embarrassed, but she couldn't help but laugh along with her friends. They teased her for being so sneaky, but also admired her for her bravery and quick thinking. Emily just smiled and shrugged, happy to be surrounded by such good friends.\\n\\nDespite the mishap, Emily and her friends had a wonderful day at the pool. They swam and played, laughed and joked, making memories that would last a lifetime. And as they left the pool that evening, Emily knew that she would always remember the day she took a sly pee on the floor and her friends found it hilarious. \n",
      " \n",
      " label: 1\n",
      "Decoded string for example 10: User prompt: Summarize the HTML below.\\nThe given html is: \\\"<p><a href=\\\"#service-collapse-1\\\">Gas Station<\\/a><\\/p> <p>Gas Hours<\\/p> <p>Mon-Fri.<\\/p> <p>6:00am - 9:30pm<\\/p> <p>Sat.<\\/p> <p>7:00am - 7:00pm<\\/p> <p>Sun.<\\/p> <p>7:00am - 7:00pm<\\/p> <p>Labor Day<\\/p> <p>Closed<\\/p> <p>Regular $3.929<\\/p> <p>Premium $4.249<\\/p> <p>Diesel $4.459<\\/p> <p>Prices shown here are updated frequently, but may not reflect the price at the pump at the time of purchase. All sales will be made at the price posted on the pumps at each Costco location at the time of purchase.<\\/p>\\\".\\n Report the gas prices.\n",
      "\n",
      "Model A :\n",
      "Here's a summary of the given HTML:\\n\\n* A link to a section named \\\"service-collapse-1\\\" with the text \\\"Gas Station\\\"\\n* A paragraph with the text \\\"Gas Hours\\\"\\n* A paragraph with the text \\\"Mon-Fri.\\\" and a time range of 6:00am - 9:30pm\\n* A paragraph with the text \\\"Sat.\\\" and a time range of 7:00am - 7:00pm\\n* A paragraph with the text \\\"Sun.\\\" and a time range of 7:00am - 7:00pm\\n* A paragraph with the text \\\"Labor Day\\\" and the text \\\"Closed\\\"\\n* A paragraph with the text \\\"Regular $3.929\\\"\\n* A paragraph with the text \\\"Premium $4.249\\\"\\n* A paragraph with the text \\\"Diesel $4.459\\\"\\n* A paragraph that mentions prices are updated frequently but may not reflect the price at the pump at the time of purchase, and that all sales will be made at the price posted on the pumps at each Costco location at the time of purchase. Sure, here are the gas prices mentioned in the HTML:\\n\\n* Regular: $3.929\\n* Premium: $4.249\\n* Diesel: $4.459\n",
      "\n",
      "--------\n",
      "\n",
      "Model B:\n",
      "The provided HTML contains information about a gas station. It provides details about the service hours from Monday to Sunday. On weekdays, it is open from 6:00am to 9:30pm. On weekends, the station operates from 7:00am to 7:00pm. However, the station is closed on Labor Day. The current prices for regular, premium, and diesel gas are $3.929, $4.249, and $4.459 respectively. The HTML also contains a disclaimer that the prices are updated frequently and the actual price at the time of purchase may vary. The final sales price will be the one posted on the pumps at each Costco location at the time of purchase. The gas prices reported in the HTML are:\\n\\n- Regular gas: $3.929\\n- Premium gas: $4.249\\n- Diesel: $4.459 \n",
      " \n",
      " label: 1\n",
      "Decoded string for example 11: User prompt: Which character is known for the phrase \\\"Hello there!\\\"?\n",
      "\n",
      "Model A :\n",
      "The character known for the phrase \\\"Hello there!\\\" is Elmo from Sesame Street.\n",
      "\n",
      "--------\n",
      "\n",
      "Model B:\n",
      "The character known for the phrase \\\"Hello there!\\\" is Obi-Wan Kenobi from the Star Wars franchise. \n",
      " \n",
      " label: 1\n",
      "Decoded string for example 12: User prompt: The father of Maria has 5 daughters. Ana, Irina, Nina and Alina. What is the name of the fifth daughter?\n",
      "\n",
      "Model A :\n",
      "<div><div><\\/div><\\/div>\\n<div><div><div><div><p>Maria's father has 5 daughters, and the name of the fifth daughter is not specified.<\\/p><\\/div><\\/div><\\/div><div>\\n<\\/div><\\/div>\n",
      "\n",
      "--------\n",
      "\n",
      "Model B:\n",
      "The name of the fifth daughter is Maria, as mentioned in the beginning of the sentence. \n",
      " \n",
      " label: 2\n",
      "Decoded string for example 13: User prompt: i need a  detailed daily schedule for a person with ADHD. I have a dog and walk it twice a day (about 8am and 6 pm) every walk takes an hour. I have hard time focusing, staying motivated and productive. i am looking for a job right now, so i need to prepare for an interview, code apps to add to my portfolio and spend some time applying for jobs and networking. in the end of the day i need to prepare my working space for the next day (takes about 15 minutes). before i start working i need to spend time to make a to-do list on a project i will code that day (takes about 15 minutes). also i need tips on how to stay focused, productive and motivated (like listening to brown noise during work).\n",
      "\n",
      "Model A :\n",
      "Here is a detailed daily schedule that takes into account your needs and goals:\\n\\n7:00 am - Wake up and get ready for the day. Take any necessary medications or supplements.\\n\\n7:30 am - Take your dog for a walk. This will provide exercise for both you and your dog, and help you start the day on a positive note.\\n\\n8:30 am - Have breakfast and take some time for yourself. You can use this time to relax, read, meditate, or engage in any activities that help you feel centered and focused.\\n\\n9:00 am - Start your job search and networking activities. Spend time researching potential job opportunities, updating your resume, and reaching out to contacts in your field.\\n\\n10:30 am - Take a short break to stretch, hydrate, and refocus your mind. Use this time to clear your head and recharge.\\n\\n10:45 am - Begin coding and working on your portfolio projects. Use the to-do list you prepared the previous day to guide your work. Break down tasks into smaller, manageable chunks to prevent feeling overwhelmed.\\n\\n12:30 pm - Take a break for lunch. Eat a nutritious meal and take some time to relax and recharge. Consider engaging in a physical activity, like a short walk or stretching, to help improve focus and energy levels.\\n\\n1:30 pm - Resume coding and working on your projects. Use techniques to stay focused, such as listening to brown noise or instrumental music, setting specific goals for each coding session, and using time management techniques like the Pomodoro Technique (working in focused bursts with short breaks in between).\\n\\n3:30 pm - Take a short break to stretch, move around, and refocus your mind. Use this time to check your email or respond to any urgent messages.\\n\\n3:45 pm - Continue working on your projects, focusing on the tasks at hand. Stay motivated by reminding yourself of your long-term goals and the benefits of completing your projects.\\n\\n5:45 pm - Finish up your coding session and start preparing your workspace for the next day. Clear your desk, organize any materials, and make sure everything is ready for a productive start tomorrow.\\n\\n6:00 pm - Take your dog for the evening walk. This will provide another opportunity for exercise and fresh air, and help you transition from work mode to relaxation mode.\\n\\n7:00 pm - Spend time engaging in activities that help you unwind and relax. This could include reading, watching a movie, or practicing a hobby you enjoy. This time is important for recharging your mind and body.\\n\\n9:00 pm - Start winding down for bed. Establish a relaxing bedtime routine that includes activities like reading, journaling, or practicing mindfulness. Avoid screens (TV, computer, phone) at least 30 minutes before sleep, as they can disrupt sleep quality.\\n\\n10:00 pm - Aim to be in bed, allowing yourself enough time to get a full night's rest and wake up refreshed the next day.\\n\\nIn addition to this schedule, here are some tips to help you stay focused, productive, and motivated:\\n\\n1. Create a dedicated workspace: Set up a designated area for work that is free from distractions and clutter. This will help signal your brain that it's time to focus.\\n\\n2. Break tasks into smaller chunks: Breaking down tasks into smaller, more manageable steps can make them feel less overwhelming and increase your motivation to start and complete them.\\n\\n3. Use timers and reminders: Set timers to stay on track with your schedule and remind yourself to take breaks, switch tasks, or refocus your mind.\\n\\n4. Prioritize self-care: Make sure to prioritize self-care activities, such as exercise, healthy eating, and getting enough sleep. Taking care of your physical and mental well-being will help improve focus and overall productivity.\\n\\n5. Experiment with different focus techniques: Explore different techniques that help you stay focused, such as the Pomodoro Technique, listening to background noise or music, or using productivity apps or tools.\\n\\nRemember, everyone's needs and preferences are different, so feel free to \n",
      " \n",
      " label: 0\n",
      "Decoded string for example 14: User prompt: Stephen has 2 apples. Yesterday he had 1 apple. How many apples does he have today?\n",
      "\n",
      "Model A :\n",
      "Stephen has 2 apples today.\n",
      "\n",
      "--------\n",
      "\n",
      "Model B:\n",
      "Stephen has 2 apples today, as he had 1 apple yesterday. \n",
      " \n",
      " label: 0\n",
      "Decoded string for example 15: User prompt: Write Python code in which function f that calls function g that calls function h that calls function f. \n",
      "\n",
      "Model A :\n",
      "Here is Python code with functions f, g, and h that call each other recursively:\\n\\n```python\\ndef f():\\n  print(\\\"Inside f\\\")\\n  g()\\n\\ndef g():\\n  print(\\\"Inside g\\\")\\n  h()\\n  \\ndef h():\\n  print(\\\"Inside h\\\")\\n  f()\\n\\nf()\\n```\\n\\nThis defines three functions - f, g, and h. \\n\\nFunction f prints \\\"Inside f\\\" and then calls function g.\\n\\nFunction g prints \\\"Inside g\\\" and calls function h.\\n\\nFunction h prints \\\"Inside h\\\" and calls function f, completing the cycle.\\n\\nThe last line f() starts the recursive calling cycle by calling function f initially.\\n\\nThe output will be:\\n\\n```\\nInside f\\nInside g \\nInside h\\nInside f\\nInside g\\nInside h\\n...\\n```\\n\\nAnd so on recursively until the recursion depth is reached and a stack overflow occurs.\n",
      "\n",
      "--------\n",
      "\n",
      "Model B:\n",
      "Here is an example of Python code where function `f` calls function `g` that calls function `h` that calls function `f`:\\n\\n```python\\ndef f():\\n    print(\\\"Inside f\\\")\\n    g()\\n\\ndef g():\\n    print(\\\"Inside g\\\")\\n    h()\\n\\ndef h():\\n    print(\\\"Inside h\\\")\\n    f()\\n\\n# Call function f\\nf()\\n```\\n\\nWhen you run this code, it will output:\\n\\n```\\nInside f\\nInside g\\nInside h\\nInside f\\n```\\n\\nAs you can see, function `f` calls function `g`, which then calls function `h`, and finally function `h` calls function `f` again, creating a cycle of function calls. \n",
      " \n",
      " label: 1\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T14:54:02.740524Z",
     "start_time": "2025-08-14T14:54:02.734598Z"
    }
   },
   "cell_type": "code",
   "source": "label",
   "id": "1c964b46ec019ad1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 0, 0, 1]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model.train()\n",
    "\n",
    "# Loss function, Cross Entropy\n",
    "LOSS_FN = torch.nn.CrossEntropyLoss().to(device=device, dtype=torch.float32)\n",
    "st = time()\n",
    "warnings.filterwarnings(\"error\")\n",
    "METRICS = {\n",
    "    \"loss\": [],\n",
    "    'accuracy': {'y_true': [], 'y_pred': []}\n",
    "}\n",
    "\n",
    "print(f'CFG.NUM_EPOCHS={CFG.NUM_EPOCH}, STEPS_PER_EPOCH={STEPS_PER_EPOCH}')\n",
    "\n",
    "for epoch in tqdm(range(CFG.NUM_EPOCH)):\n",
    "    ste = time()\n",
    "    for step in range(STEPS_PER_EPOCH):\n",
    "        # Zero out Gradient\n",
    "        OPTIMIZER.zero_grad()\n",
    "\n",
    "        input_ids, attention_mask, labels = next(TRAIN_DATASET)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        logits = outputs.logits.to(dtype=torch.float32)\n",
    "\n",
    "        loss = LOSS_FN(logits, labels.to(dtype=torch.float32))\n",
    "        loss.backward()\n",
    "\n",
    "        OPTIMIZER.step()\n",
    "\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        METRICS['loss'].append(float(loss))\n",
    "        METRICS['accuracy']['y_true'] += labels.squeeze().tolist()\n",
    "        METRICS['accuracy']['y_pred'] += torch.argmax(F.softmax(logits, dim=-1), dim=-1).cpu().tolist()\n",
    "\n",
    "        if (step + 1) % 200 == 0:\n",
    "            metrics = 'µ_loss: {:.3f}'.format(np.mean(METRICS['loss']))\n",
    "            metrics += ', step_loss: {:.3f}'.format(METRICS['loss'][-1])\n",
    "            metrics += ', µ_auc: {:.3f}'.format(\n",
    "                accuracy_score(torch.argmax(torch.tensor(METRICS['accuracy']['y_true']), dim=-1),\n",
    "                               METRICS['accuracy']['y_pred']))\n",
    "            lr = OPTIMIZER.param_groups[0]['lr']\n",
    "            print(f'{epoch + 1:02}/{CFG.NUM_EPOCH:02} | {step + 1:04}/{STEPS_PER_EPOCH} lr: {lr:.2E}, {metrics}',\n",
    "                  end='')\n",
    "            print(f'\\nSteps per epoch: {step + 1} complete | Time elapsed: {time() - st}')\n",
    "    print(f'\\nEpoch {epoch + 1} Completed | Total time for epoch: {time() - ste} ')\n",
    "\n",
    "    torch.save(\n",
    "        {k: v.cpu() for k, v in model.named_parameters() if v.requires_grad},\n",
    "        f'model_llama_3_cp_{epoch + 1}_v1.pth'\n",
    "    )\n",
    "\n",
    "    torch.save(\n",
    "        OPTIMIZER.state_dict(),\n",
    "        f'optimizer_llama_3_cp_{epoch + 1}_v1.pth'\n",
    "    )\n"
   ],
   "id": "fce531dcf5f2b5fd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
