{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## this notebook from https://www.kaggle.com/code/imaadmahmood/llm-finetuning",
   "id": "54a92c8b64265e6"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-13T10:16:16.696225Z",
     "start_time": "2025-08-13T10:16:16.403174Z"
    }
   },
   "source": [
    "# import zipfile\n",
    "# import os\n",
    "# from pyexpat import features\n",
    "#\n",
    "# from lark.tools import flags\n",
    "#\n",
    "# zip_path = \"../../llm-classification-finetuning.zip\"\n",
    "# extract_path = \"../../kaggle\"\n",
    "#\n",
    "# # Create folder if it doesn't exist\n",
    "# os.makedirs(extract_path, exist_ok=True)\n",
    "#\n",
    "# # Open and extract\n",
    "# with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "#     zip_ref.extractall(extract_path)\n",
    "#\n",
    "# print(f\"Unzipped to {extract_path}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unzipped to ../kaggle\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T10:16:16.722016Z",
     "start_time": "2025-08-13T10:16:16.709182Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "sample_submission = pd.read_csv(\"../../kaggle/LLM_Classification_FineTuning/sample_submission.csv\")\n",
    "sample_submission"
   ],
   "id": "88713a14235af159",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        id  winner_model_a  winner_model_b  winner_tie\n",
       "0   136060        0.333333        0.333333    0.333333\n",
       "1   211333        0.333333        0.333333    0.333333\n",
       "2  1233961        0.333333        0.333333    0.333333"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>winner_model_a</th>\n",
       "      <th>winner_model_b</th>\n",
       "      <th>winner_tie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>136060</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>211333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1233961</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T10:16:17.945463Z",
     "start_time": "2025-08-13T10:16:16.759371Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_df = pd.read_csv(\"../../kaggle/LLM_Classification_FineTuning/test.csv\")\n",
    "train_df = pd.read_csv(\"../../kaggle/LLM_Classification_FineTuning/train.csv\")\n",
    "\n",
    "train_df.head(2)"
   ],
   "id": "4e48384d9c39e6e4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      id             model_a     model_b  \\\n",
       "0  30192  gpt-4-1106-preview  gpt-4-0613   \n",
       "1  53567           koala-13b  gpt-4-0613   \n",
       "\n",
       "                                              prompt  \\\n",
       "0  [\"Is it morally right to try to have a certain...   \n",
       "1  [\"What is the difference between marriage lice...   \n",
       "\n",
       "                                          response_a  \\\n",
       "0  [\"The question of whether it is morally right ...   \n",
       "1  [\"A marriage license is a legal document that ...   \n",
       "\n",
       "                                          response_b  winner_model_a  \\\n",
       "0  [\"As an AI, I don't have personal beliefs or o...               1   \n",
       "1  [\"A marriage license and a marriage certificat...               0   \n",
       "\n",
       "   winner_model_b  winner_tie  \n",
       "0               0           0  \n",
       "1               1           0  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>model_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "      <th>winner_model_a</th>\n",
       "      <th>winner_model_b</th>\n",
       "      <th>winner_tie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30192</td>\n",
       "      <td>gpt-4-1106-preview</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>[\"Is it morally right to try to have a certain...</td>\n",
       "      <td>[\"The question of whether it is morally right ...</td>\n",
       "      <td>[\"As an AI, I don't have personal beliefs or o...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53567</td>\n",
       "      <td>koala-13b</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>[\"What is the difference between marriage lice...</td>\n",
       "      <td>[\"A marriage license is a legal document that ...</td>\n",
       "      <td>[\"A marriage license and a marriage certificat...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T10:16:17.989463Z",
     "start_time": "2025-08-13T10:16:17.987019Z"
    }
   },
   "cell_type": "code",
   "source": "train_df.columns.to_list()",
   "id": "f53d56344829d168",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'model_a',\n",
       " 'model_b',\n",
       " 'prompt',\n",
       " 'response_a',\n",
       " 'response_b',\n",
       " 'winner_model_a',\n",
       " 'winner_model_b',\n",
       " 'winner_tie']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T10:16:18.063463Z",
     "start_time": "2025-08-13T10:16:18.060386Z"
    }
   },
   "cell_type": "code",
   "source": "train_df.dtypes",
   "id": "a1fa830e07f33a57",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                 int64\n",
       "model_a           object\n",
       "model_b           object\n",
       "prompt            object\n",
       "response_a        object\n",
       "response_b        object\n",
       "winner_model_a     int64\n",
       "winner_model_b     int64\n",
       "winner_tie         int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T10:36:16.896680Z",
     "start_time": "2025-08-13T10:36:16.882164Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import log_loss\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "class OptimizedLLMPredictor:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the predictor with optimized setting\"\"\"\n",
    "        self.label_encoders = {}\n",
    "        self.scaler = StandardScaler()\n",
    "        self.models = {}\n",
    "        self.feature_names = []\n",
    "\n",
    "        print(\"OptimizedLLMPredictor Initialized!\")\n",
    "\n",
    "    def extract_fast_features(self, df: pd.DataFrame, is_train=True):\n",
    "        \"\"\"Extract optimized features with focus on speed and performance\"\"\"\n",
    "\n",
    "        print(\"Starting feature engineering...\")\n",
    "        features: pd.DataFrame = df[['id']].copy()\n",
    "\n",
    "        for resp in ['response_a', 'response_b']:\n",
    "            text_col = df[resp].astype(str)\n",
    "\n",
    "            # Basic length metrics\n",
    "            features[f'{resp}_len'] = text_col.str.len()\n",
    "            features[f'{resp}_words'] = text_col.str.split().str.len()\n",
    "            features[f'{resp}_sentences'] = text_col.str.count(r'[.!?]+')\n",
    "\n",
    "            # Advanced metrics\n",
    "            features[f'{resp}_avg_word_len'] = features[f'{resp}_len'] / (features[f'{resp}_words'] + 1)\n",
    "            features[f'{resp}_punct_ratio'] = text_col.str.count(r'\\w\\s') / (features[f'{resp}_len'] + 1)\n",
    "            features[f'{resp}_upper_ratio'] = text_col.str.count(r'[A-Z]') / (features[f'{resp}_len'] + 1)\n",
    "\n",
    "            # Structure indicators\n",
    "            features[f'{resp}_newlines'] = text_col.str.count(r\"\\n\")\n",
    "            features[f'{resp}_code_blocks'] = text_col.str.count(r\"```\")\n",
    "            features[f'{resp}_bullets'] = text_col.str.count(r'^\\s*[-*â€¢]\\s', flags=re.MULTILINE)\n",
    "            features[f'{resp}_numbers'] = text_col.str.count(r'^\\s*\\d+\\.\\s', flags=re.MULTILINE)\n",
    "\n",
    "            # Quality indicators\n",
    "            features[f'{resp}_questions'] = text_col.str.count(r'\\?')\n",
    "            features[f'{resp}_exclamations'] = text_col.str.count(r'\\!')\n",
    "        print(\"Basic text features extracted\")\n",
    "\n",
    "        prompt_text = df['prompt'].astype(str)\n",
    "        features['prompt_len'] = prompt_text.str.len()\n",
    "        features['prompt_words'] = prompt_text.str.split().str.len()\n",
    "        features['prompt_questions'] = prompt_text.str.count(r'\\?')\n",
    "\n",
    "        print(\"Prompt features extracted\")\n",
    "\n",
    "        # Length comparisons\n",
    "        features['len_ratio_a_b'] = features['response_a_len'] / features['response_b_len']\n",
    "        features['len_diff_a_b'] = features['response_a_len'] - features['response_b_len']\n",
    "        features['word_ratio_a_b'] = features['response_a_words'] / (features['response_b_words'] + 1)\n",
    "        features['word_diff_a_b'] = features['response_a_words'] - features['response_b_words']\n",
    "\n",
    "        # Quality comparisons\n",
    "        features['struct_diff_a_b'] = (features['response_a_bullets'] + features['response_a_numbers']) - (\n",
    "                features['response_b_bullets'] + features['response_b_numbers'])\n",
    "        features['engagement_diff_a_b'] = (features['response_a_questions'] + features['response_a_exclamations']) - (\n",
    "                features['response_b_questions'] + features['response_b_exclamations'])\n",
    "        print(\"Comparative features extracted\")\n",
    "\n",
    "        if is_train and 'model_a' in df.columns:\n",
    "            # Fast model encoding\n",
    "            for model_col in ['model_a', 'model_b']:\n",
    "                if model_col not in self.label_encoders:\n",
    "                    self.label_encoders[model_col] = LabelEncoder()\n",
    "                    features[f'{model_col}_id'] = self.label_encoders[model_col].fit_transform(df[model_col])\n",
    "                    features[f'{model_col}_id'] = self.label_encoders[model_col].transform(df[model_col])\n",
    "                else:\n",
    "                    features[f'{model_col}_id'] = self.label_encoders[model_col].transform(df[model_col])\n",
    "\n",
    "            # Quick model stats\n",
    "            model_wins = df.groupby('model_a')['winner_model_a'].mean()\n",
    "            model_wins_b = df.groupby('model_b')['winner_model_b'].mean()\n",
    "\n",
    "            features['model_a_win_rate'] = df['model_a'].map(model_wins).fillna(0.33)\n",
    "            features['model_b_win_rate'] = df['model_b'].map(model_wins_b).fillna(0.33)\n",
    "\n",
    "            print(\"Model features extracted\")\n",
    "        elif not is_train:\n",
    "            # For test data, create dummy model features to maintain consistency\n",
    "            features['model_a_id'] = 0  # default encoding for unknown models\n",
    "            features['model_b_id'] = 0\n",
    "            features['model_a_win_rate'] = 0.33  # default win rate\n",
    "            features['model_b_win_rate'] = 0.33\n",
    "\n",
    "            print(\"Dummy model features added for test consistency\")\n",
    "\n",
    "        def fast_word_overlap(row):\n",
    "            words_a = set(str(row['response_a']).lower().split())\n",
    "            words_b = set(str(row['response_b']).lower().split())\n",
    "            if len(words_a) == 0 or len(words_b) == 0:\n",
    "                return 0\n",
    "            return len(words_a & words_b) / len(words_a | words_b)\n",
    "\n",
    "        features['word_overlap'] = df.apply(fast_word_overlap, axis=1)\n",
    "\n",
    "        print(\"Similarity features extracted\")\n",
    "\n",
    "        features = features.fillna(0)\n",
    "        self.feature_names = [col for col in features.columns if col != 'id']\n",
    "\n",
    "        print(f\" Feature engineering completed! Total features: {len(self.feature_names)}\")\n",
    "\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        return features\n",
    "\n",
    "    def train_optimized_ensemble(self, X, y):\n",
    "        \"\"\"Train optimized ensemble with focus on speed and performance\"\"\"\n",
    "\n",
    "        print(\"Training optimized ensemble models...\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        y_multiclass = np.argmax(y.values, axis=1)\n",
    "\n",
    "        # Model 1: LightGBM (Primary Model)\n",
    "        print(\"Training LightGBM (Primary Model) ...\")\n",
    "        self.models['lgb'] = lgb.LGBMClassifier(\n",
    "            n_estimators=800,\n",
    "            learning_rate=0.08,\n",
    "            max_depth=6,\n",
    "            num_leaves=31,\n",
    "            subsample=0.9,\n",
    "            colsample_bytree=0.9,\n",
    "            random_state=42,\n",
    "            objective='multiclass',\n",
    "            num_class=3,\n",
    "            verbose=-1,\n",
    "            force_col_wise=True  # Optimization for speed\n",
    "        )\n",
    "        self.models['lgb'].fit(X, y_multiclass)\n",
    "\n",
    "        # Model 2: XGBoost (Secondary Model)\n",
    "        print(\"Training XGBoost\")\n",
    "        self.models['xgb'] = xgb.XGBClassifier(\n",
    "            n_estimators=600,\n",
    "            learning_rate=0.08,\n",
    "            max_depth=6,\n",
    "            subsample=0.9,\n",
    "            colsample_bytree=0.9,\n",
    "            random_state=42,\n",
    "            objective='multi:softprob',\n",
    "            eval_metric='mlogloss',\n",
    "            verbosity=0,\n",
    "            tree_method='hist'  # Faster training\n",
    "        )\n",
    "        self.models['xgb'].fit(X, y_multiclass)\n",
    "        print(\"XGBoost training completed!\")\n",
    "\n",
    "        # Model 3: CatBoost (Robust Model)\n",
    "        print(\"Training CatBoost...\")\n",
    "        self.models['catboost'] = CatBoostClassifier(\n",
    "            iterations=500,\n",
    "            learning_rate=0.1,\n",
    "            depth=6,\n",
    "            random_state=42,\n",
    "            verbose=False,\n",
    "            loss_function='MultiClass',\n",
    "            task_type=\"CPU\",\n",
    "            logging_level='Silent',\n",
    "            train_dir=None  # No log files\n",
    "        )\n",
    "        self.models['catboost'].fit(X, y_multiclass)\n",
    "        print(\"CatBoost training completed!\")\n",
    "\n",
    "        # Ensemble Weights (Optimized)\n",
    "        self.ensemble_weights = {\n",
    "            'lgb': 0.5,  # Primary model\n",
    "            'xgb': 0.35,  # Strong secondary\n",
    "            'catboost': 0.15  # Robustness\n",
    "        }\n",
    "\n",
    "        print(\" Ensemble training completed!\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "    def predict_optimized(self, X):\n",
    "        \"\"\"Make optimized ensemble predictions with feature validation\"\"\"\n",
    "\n",
    "        print(\"Making ensemble predictions...\")\n",
    "\n",
    "        if hasattr(self, 'feature_names'):\n",
    "            if X.shape[1] != len(self.feature_names):\n",
    "                print(f\"Feature mismatch detected!\")\n",
    "                print(f\"Expected {len(self.feature_names)} features\")\n",
    "                print(f\"Received: {X.shape[1]} features\")\n",
    "\n",
    "                # Ensure X has the same columns as training\n",
    "                if isinstance(X, pd.DataFrame):\n",
    "                    missing_features = set(self.feature_names) - set(X.columns)\n",
    "                    extra_features = set(X.columns) - set(self.feature_names)\n",
    "\n",
    "                    if missing_features:\n",
    "                        print(f\"Adding missing feature: {missing_features}\")\n",
    "                        for feature in missing_features:\n",
    "                            X[feature] = 0\n",
    "\n",
    "                    if extra_features:\n",
    "                        print(f\"Removing extra features: {extra_features}\")\n",
    "                        X = X.drop(columns=list(extra_features))\n",
    "\n",
    "                    # Reorder columns to match training\n",
    "                    X = X[self.feature_names]\n",
    "        print(f\"Feature validation completed. Shape {X.shape}\")\n",
    "\n",
    "        predictions = np.zeros((X.shape[0], 3))\n",
    "\n",
    "        for name, model in self.models.items():\n",
    "            pred = model.predict_proba(X)\n",
    "            predictions += self.ensemble_weights[name] * pred\n",
    "            print(f\"{name} predictions added (weight: {self.ensemble_weights[name]}\")\n",
    "\n",
    "        print(\"Ensemble predictions completed!\")\n",
    "        print(\"=\" * 60)\n",
    "        return predictions\n",
    "\n",
    "    def quick_validation(self, X, y, n_splits=3):\n",
    "        \"\"\"Quick cross-validation for performance check\"\"\"\n",
    "\n",
    "        print(\"Running quick validation\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        y_multiclass = np.argmax(y.values, axis=1)\n",
    "        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "        # Only validate primary model for speed\n",
    "        model = self.models['lgb']\n",
    "        scores = []\n",
    "\n",
    "        for fold, (train_idx, val_idx) in enumerate(skf.split(X, y_multiclass)):\n",
    "            X_fold_train, X_fold_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "            y_fold_train, y_fold_val = y_multiclass[train_idx], y_multiclass[val_idx]\n",
    "\n",
    "            # Quick training\n",
    "            fold_model = lgb.LGBMClassifier(\n",
    "                n_estimators=200, learning_rate=0.1, max_depth=6,\n",
    "                random_state=42, verbose=-1, objective='multiclass', num_class=3\n",
    "            )\n",
    "\n",
    "            fold_model.fit(X_fold_train, y_fold_train)\n",
    "\n",
    "            # Prediction and scoring\n",
    "            pred_proba = fold_model.predict_proba(X_fold_val)\n",
    "            y_val_onehot = np.eye(3)[y_fold_val]\n",
    "            score = log_loss(y_val_onehot, pred_proba)\n",
    "            scores.append(score)\n",
    "\n",
    "            print(f\"Fold {fold+1} Log Loss: {score:.4f}\")\n",
    "        avg_score = np.mean(scores)\n",
    "        std_score = np.std(scores)\n",
    "\n",
    "        print(f\"Average CV Score: {avg_score:.4f} (+/- {std_score: .4f})\")\n",
    "        print(\"=\" * 60)\n",
    "        return avg_score"
   ],
   "id": "df36771e330f6ebb",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T10:36:17.587521Z",
     "start_time": "2025-08-13T10:36:17.582416Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    \"\"\"Optimized main pipeline for maximum performance\"\"\"\n",
    "\n",
    "    print(\"STARTING LLM PREFERENCE PREDICTION PIPELINE\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    print(\"Loading competition data...\")\n",
    "    test_df = pd.read_csv(\"../../kaggle/LLM_Classification_FineTuning/test.csv\")\n",
    "    train_df = pd.read_csv(\"../../kaggle/LLM_Classification_FineTuning/train.csv\")\n",
    "    sample_submission = pd.read_csv('../../kaggle/LLM_Classification_FineTuning/sample_submission.csv')\n",
    "\n",
    "    print(f\"Train shape: {train_df.shape}\")\n",
    "    print(f\"Test shape: {test_df.shape}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    predictor = OptimizedLLMPredictor()\n",
    "\n",
    "    train_features = predictor.extract_fast_features(train_df, is_train=True)\n",
    "    X_train = train_features.drop(['id'], axis=1)\n",
    "    y_train = train_df[['winner_model_a', 'winner_model_b', 'winner_tie']].copy()\n",
    "\n",
    "    # Store feature names for consistency\n",
    "    predictor.feature_names = list(X_train.columns)\n",
    "\n",
    "    print(f\"Final training shape: {X_train.shape}\")\n",
    "    print(f\"Feature names stored: {len(predictor.feature_names)} features\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    predictor.train_optimized_ensemble(X_train, y_train)\n",
    "\n",
    "    try:\n",
    "        cv_score = predictor.quick_validation(X_train, y_train)\n",
    "        performance_indicator = \"ðŸ”¥ EXCELLENT\" if cv_score < 1.05 else \"âœ… GOOD\" if cv_score < 1.10 else \"âš ï¸ NEEDS IMPROVEMENT\"\n",
    "        print(f\"Model Performance: {performance_indicator}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Validation skipped: {str(e)}\")\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(\"Processing test data...\")\n",
    "    test_features = predictor.extract_fast_features(test_df, is_train=False)\n",
    "    X_test = test_features.drop(['id'], axis=1)\n",
    "\n",
    "    print(f\"Test features shape: {X_test.shape}\")\n",
    "    print(f\"Expected shape: ({X_test.shape[0]}, {len(predictor.feature_names)})\")\n",
    "\n",
    "    # Make predictions with feature validation\n",
    "    predictions = predictor.predict_optimized(X_test)\n",
    "\n",
    "    print(\"Creating optimized submission...\")\n",
    "\n",
    "    submission = pd.DataFrame({\n",
    "        'id': test_df['id'],\n",
    "        'winner_model_a': predictions[:, 0],\n",
    "        'winner_model_b': predictions[:, 1],\n",
    "        'winner_tie': predictions[:, 2]\n",
    "    })\n",
    "\n",
    "    # Normalize probabilities to ensure they sum to 1\n",
    "    prob_sums = submission[['winner_model_a', 'winner_model_b', 'winner_tie']].sum(axis=1)\n",
    "    submission[['winner_model_a', 'winner_model_b', 'winner_tie']] = \\\n",
    "        submission[['winner_model_a', 'winner_model_b', 'winner_tie']].div(prob_sums, axis=0)\n",
    "\n",
    "    # Save submission\n",
    "    submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "    print(\"Submission saved as 'submission.csv'\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(\"SUBMISSION SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Shape: {submission.shape}\")\n",
    "    print(f\"Columns: {list(submission.columns)}\")\n",
    "\n",
    "    # Check probability distributions\n",
    "    prob_stats = submission[['winner_model_a', 'winner_model_b', 'winner_tie']].describe()\n",
    "    print(\"\\nProbability Distributions:\")\n",
    "    print(prob_stats.round(4))\n",
    "\n",
    "    print(\"\\nSample Predictions:\")\n",
    "    print(submission.head().round(4))\n",
    "\n",
    "    # Final validation\n",
    "    prob_sums_check = submission[['winner_model_a', 'winner_model_b', 'winner_tie']].sum(axis=1)\n",
    "    print(f\"\\nProbability sums (should be ~1.0): Min={prob_sums_check.min():.6f}, Max={prob_sums_check.max():.6f}\")\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"Ready for submission to leaderboard!\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    return predictor, submission"
   ],
   "id": "815d6fe302858967",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T10:36:58.839423Z",
     "start_time": "2025-08-13T10:36:18.501097Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == '__main__':\n",
    "    predictor, submission = main()"
   ],
   "id": "242fda069f5ba2f4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING LLM PREFERENCE PREDICTION PIPELINE\n",
      "============================================================\n",
      "Loading competition data...\n",
      "Train shape: (57477, 9)\n",
      "Test shape: (3, 4)\n",
      "============================================================\n",
      "OptimizedLLMPredictor Initialized!\n",
      "Starting feature engineering...\n",
      "Basic text features extracted\n",
      "Prompt features extracted\n",
      "Comparative features extracted\n",
      "Model features extracted\n",
      "Similarity features extracted\n",
      " Feature engineering completed! Total features: 38\n",
      "============================================================\n",
      "Final training shape: (57477, 38)\n",
      "Feature names stored: 38 features\n",
      "============================================================\n",
      "Training optimized ensemble models...\n",
      "============================================================\n",
      "Training LightGBM (Primary Model) ...\n",
      "Training XGBoost\n",
      "XGBoost training completed!\n",
      "Training CatBoost...\n",
      "CatBoost training completed!\n",
      " Ensemble training completed!\n",
      "============================================================\n",
      "Running quick validation\n",
      "============================================================\n",
      "Fold 1 Log Loss: 1.0078\n",
      "Fold 2 Log Loss: 1.0046\n",
      "Fold 3 Log Loss: 1.0063\n",
      "Average CV Score: 1.0062 (+/-  0.0013)\n",
      "============================================================\n",
      "Model Performance: ðŸ”¥ EXCELLENT\n",
      "============================================================\n",
      "Processing test data...\n",
      "Starting feature engineering...\n",
      "Basic text features extracted\n",
      "Prompt features extracted\n",
      "Comparative features extracted\n",
      "Dummy model features added for test consistency\n",
      "Similarity features extracted\n",
      " Feature engineering completed! Total features: 38\n",
      "============================================================\n",
      "Test features shape: (3, 38)\n",
      "Expected shape: (3, 38)\n",
      "Making ensemble predictions...\n",
      "Feature validation completed. Shape (3, 38)\n",
      "lgb predictions added (weight: 0.5\n",
      "xgb predictions added (weight: 0.35\n",
      "catboost predictions added (weight: 0.15\n",
      "Ensemble predictions completed!\n",
      "============================================================\n",
      "ðŸ“ Creating optimized submission...\n",
      "Submission saved as 'submission.csv'\n",
      "============================================================\n",
      "SUBMISSION SUMMARY\n",
      "============================================================\n",
      "Shape: (3, 4)\n",
      "Columns: ['id', 'winner_model_a', 'winner_model_b', 'winner_tie']\n",
      "\n",
      "Probability Distributions:\n",
      "       winner_model_a  winner_model_b  winner_tie\n",
      "count          3.0000          3.0000      3.0000\n",
      "mean           0.3360          0.2860      0.3780\n",
      "std            0.1474          0.1398      0.2673\n",
      "min            0.1684          0.1504      0.1760\n",
      "25%            0.2814          0.2142      0.2264\n",
      "50%            0.3944          0.2779      0.2768\n",
      "75%            0.4198          0.3537      0.4790\n",
      "max            0.4453          0.4296      0.6812\n",
      "\n",
      "Sample Predictions:\n",
      "        id  winner_model_a  winner_model_b  winner_tie\n",
      "0   136060          0.1684          0.1504      0.6812\n",
      "1   211333          0.4453          0.2779      0.2768\n",
      "2  1233961          0.3944          0.4296      0.1760\n",
      "\n",
      "Probability sums (should be ~1.0): Min=1.000000, Max=1.000000\n",
      "================================================================================\n",
      "PIPELINE COMPLETED SUCCESSFULLY!\n",
      "Ready for submission to leaderboard!\n",
      "================================================================================\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9226a4a9701edb3e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
